LGBM Parameters
1. num_iteration(n_iter, num_tree, num_trees, num_round, num_rounds, num_boost_round, n_estimators 모두 같은 것)
: 기본값은 역시 100, 1000 정도 해 주는 게 좋다?(개인 블로거의 의견)

2. learning_rate : 일반적으로 0.01 ~ 0.1 정도로 맞추고 다른 파라미터들을 튜닝, 나중에 성능을 높일 때 learning rate를 더 줄인다

3. max_depth : -1로 설정하면 제한없이 분기한다(?), feature가 많다면 크게 설정, 파라미터 설정 시 우선적으로 설정한다

4. boosting : gbdt, rf, dart, goss 
(부팅 방법, 디폴트는 gbdt, 정확도가 중요할 때는 dart사용(딥러닝의 드랍아웃과 같다), goss는 샘플링을 이용하는 것)

5. bagging_fraction : bagging을 하기 위해 데이터를 랜덤 샘플링하여 학습에 사용, 비욜은 0 < fraction <=1 , 0이 되지 않게 해야 함

6. feature_fraction : feature_fraction이 1보다 작다면 LGBM은 매 iteration(tree)마다 다른 feature를 랜덤하게 추출하여 학습
-> aksdir 0.8로 값을 설정하면 매 tree를 구성할 때 feature의 80%만 랜덤하게 선택한다, 과적합 방지 가능, 학습 속도 향상

7. scale_pos_weight : 클래스 불균형의 데이터 셋에서 weight를 주는 방식으로 positive를 증가, 기본값은 1, 불균형의 정도에 따라 조절

8. early_stopping_round : Validation set에서 평가지표가 더 이상 향상되지 않으면 학습을 정지

LGBM 파라미터 튜닝 팁
- 더 빠른 속도 
1) bagging_fraction, max_bin은 작게
2) save_binary를 쓰면 데이터 로딩속도가 빨라짐
3) parallel learning 사용

- 더 높은 정확도
1) max_bin을 크게
2) num_iterations는 크게 하고 learning_rate는 작게
3) num_leaves를 크게(과적합의 원인이 될 수도)

- 과적합 줄이기
1) max_bin을 작게
2) num_leaves를 작게
3) min_data_in_leaf와 min_sum_hessian_in_leaf

파라미터 보충 추가
Dataset Parameters
max_bin : default 255 (small number of bins may reduce training accuracy but may increase general power(deal with over-fitting))
