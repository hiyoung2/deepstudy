5. 서포트 벡터 머신

from sklearn.svm import LinearSVC, SVC, LinearSVR, SVR,

서포트 벡터 머신, Support Vector Machine(SVM)은 매우 강력하고 선형이나 비선형
분류, 회귀, 이상치 탐색에도 사용할 수 있는 다목적 머신러닝 모델이다

머신러닝에서 가장 인기 있는 모델에 속하고 머신러닝에 관심 있는 사람이라면 반드시 알고 있어야 하는 모델
SVM은 특히 복잡한 분류 문제에 잘 들어맞으며 작거나 중간 크기의 데이터셋에 적합하다

서포트 벡터 머신의 근본적인 아이디어는
클래스 사이에 가능한 한 가장 넓은 '도로'를 내는 것이다
다시 말해, 두 클래스를 구분하는 결정 경계와 샘플 사이의 마진을 가능한 한 가장 크게 하는 것이 목적
그래서 SVM 분류기를 'Large Margin Classification' 이라고 한다

여기서 서포트 벡터란, SVM이 훈련된 후에 경계를 포함해 도로에 놓인 어떤 샘플이다
결정 경계는 전적으로 서포트 벡터에 의해 결정된다
서포트 벡터가 아닌(즉, 도로 밖에 있는) 어떤 샘플도 영향을 주지 못한다
이런 샘플은 삭제하고 다른 샘플을 더 추가하거나, 다른 곳으로 이동시킬 수 있다
샘플이 도로 밖에 있는 한, 결정 경계에 영향을 주지 못한다

예측을 계산할 때는 전체 훈련 세트가 아니라 서포트 벡터만 관여된다

SVM은 특성(feature, 쉽게 말해 input data들의 컬럼별 수치에 대해 말하는 듯)의 스케일에 민감하다
특성의 스케일을 조정하면(ex. StandardScaler 등) 결정 경계가 훨씬 좋아진다

모든 샘플이 도로 바깥쪽에 올바르게 분류되어 있다면 이를 '하드 마진 분류'라고 한다
하드 마진 분류에는 두 가지 문제점이 있다
(1) 데이터가 선형적으로 구분될 수 있어야 제대로 작동
(2) 이상치에 민감

이런 문제를 피하려면 좀 더 유연한 모델이 필요하다
도로의 폭을 가능한 한 넓게 유지하는 것과 
마진오류(즉, 샘플이 도로 중간이나 심지어 반대쪽에 있는 경우) 사이에
적절한 균형을 잡아야 한다
이를 '소프트 마진 분류'라고 한다

소프트 마진 분류를 수행할 때는 SVM이 두 클래스를 완벽하게 나누는 것(1)과
가장 넓은 도로를 만드는 것(2) 사이에 절충안을 찾는다
(즉, 몇 개의 샘플은 도로 안에 놓일 수도 있다)

사이킷런의 SVM 모델을 만들 때 여러 하피퍼파라미터들을 지정할 수 있다
'C'는 이런 하이퍼파라미터 중에 하나이다
'C'를 크게 설정하면 마진오류는 적어지나 과적합 될 확률이 높다
(마진 오류는 나쁘므로 일반적으로 적은 것이 좋긴 하다)
'C'를 작게 설정하면 마진오류는 많아지지만 일반화가 더 잘 된다고 본다

SVM이 과대적합(Overfitting)이라면 C를 감소 시켜 모델을 규제할 수 있다

SVM 알고리즘은 다목적으로 사용할 수 있다
SVM을 분류가 아니라 회귀에 적용하는 방법은 목표를 반대로 하는 것이다

SVM 회귀는 제한된 마진 오류(즉, 도로 밖의 샘플) 안에서 
도로 안에 가능한 한 많은 샘플이 들어가도록 학습시킨다
도로의 폭은 하이퍼파라미터 epsilon으로 조절한다
epsilon이 클수록 마진이 크고,
epsilon이 작을수록 마진이 작다





